{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Graph Statistics:\n",
      "   - Total nodes: 4\n",
      "   - Total edges: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import Literal\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Initialize the LLM with streaming enabled\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    streaming=True  # Enable streaming at model level\n",
    ")\n",
    "\n",
    "# Define state with additional metadata\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "    message_count: int = 0  # Track number of messages\n",
    "    \n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update message count\n",
    "    new_count = state.get(\"message_count\", 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"messages\": response,\n",
    "        \"message_count\": new_count\n",
    "    }\n",
    "    \n",
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\\\n\\\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    return END\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile with memory\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Display the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Print graph statistics\n",
    "print(f\"\\n Graph Statistics:\")\n",
    "print(f\"   - Total nodes: {len(graph.get_graph().nodes)}\")\n",
    "print(f\"   - Total edges: {len(list(graph.get_graph().edges))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"Hi Riya! It's nice to meet you. How can I help you today?\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'model_provider': 'google_genai'}, id='lc_run--28ec8d68-c6a1-4427-a2d1-8084d15f4df7', usage_metadata={'input_tokens': 7, 'output_tokens': 20, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}}), 'message_count': 1}}\n",
      "\n",
      "  Response time: 1.6097137928009033 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a thread with metadata\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\",\n",
    "        \"user_id\": \"riya\", \n",
    "    }\n",
    "}\n",
    "\n",
    "# Start conversation with timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Riya\")]}, \n",
    "    config, \n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(chunk)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n  Response time: {elapsed:} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. What can I do for you today?\n",
      "Tokens - Input: 34, Output: 21\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Riya\")]}, \n",
    "    config, \n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    if \"conversation\" in chunk:\n",
    "        chunk['conversation'][\"messages\"].pretty_print()\n",
    "        \n",
    "        # Show token usage\n",
    "        if hasattr(chunk['conversation'][\"messages\"], 'usage_metadata'):\n",
    "            usage = chunk['conversation'][\"messages\"].usage_metadata\n",
    "            print(f\"Tokens - Input: {usage.get('input_tokens', 0)}, \"\n",
    "                  f\"Output: {usage.get('output_tokens', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. What can I do for you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      " Message Count: 3\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. What can I do for you today?\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I assist you today?\n",
      "\n",
      " Message Count: 4\n",
      "\n",
      "Riya:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Riya\n",
      "\n",
      "BookBot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Riya! It's nice to meet you. How can I assist you today?\n",
      "\n",
      " Message Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Start conversation with a new thread\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"2\",\n",
    "        \"user_id\": \"riya\",\n",
    "    }\n",
    "}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm Riya\")\n",
    "for event in graph.stream(\n",
    "    {\"messages\": [input_message]}, \n",
    "    config, \n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    for m in event['messages']:\n",
    "        role = \"Riya\" if m.type == \"human\" else \"BookBot\"\n",
    "        print(f\"\\n{role}:\")\n",
    "        m.pretty_print()\n",
    "    \n",
    "    # Show state metadata\n",
    "    if 'message_count' in event:\n",
    "        print(f\"\\n Message Count: {event['message_count']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatGoogleGenerativeAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n",
      "   on_chain_end                  :    3 occurrences\n",
      "   on_chain_start                :    3 occurrences\n",
      "   on_chain_stream               :    2 occurrences\n",
      "   on_chat_model_end             :    1 occurrences\n",
      "   on_chat_model_start           :    1 occurrences\n",
      "   on_chat_model_stream          :   16 occurrences\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"riya\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the major characters in the book the lies of locke lamora.\")\n",
    "\n",
    "event_counts = {}\n",
    "\n",
    "async for event in graph.astream_events(\n",
    "    {\"messages\": [input_message]}, \n",
    "    config, \n",
    "    version=\"v2\"\n",
    "):\n",
    "    event_type = event['event']\n",
    "    event_counts[event_type] = event_counts.get(event_type, 0) + 1\n",
    "    \n",
    "    node = event['metadata'].get('langgraph_node', 'root')\n",
    "    name = event.get('name', 'unknown')\n",
    "    \n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")\n",
    "\n",
    "for event_type, count in sorted(event_counts.items()):\n",
    "    print(f\"   {event_type:30}: {count:4} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='Okay', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_tokens': 1205, 'output_tokens': 0, 'total_tokens': 1205, 'input_token_details': {'cache_read': 0}})}\n",
      "{'chunk': AIMessageChunk(content=\", let'\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content='s break down the main characters in Scott Lynch\\'s \"The Lies of Locke', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=' Lamora.\" The story centers around Locke Lamora and his crew, the Gentleman Bast', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\"ards, so we'll focus on them:\\n\\n*   **Locke Lamora:** The protagonist and the brains of the operation. Locke is a charismatic\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\", quick-witted con artist with a knack for elaborate schemes and a silver tongue. He's fiercely loyal to his friends, even if his morals are a\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" bit flexible. He's not physically strong, relying instead on his intelligence and cunning. Locke is haunted by his past and his upbringing under Father Chains. He's driven by a desire for freedom and a better life, but also by a deep sense\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" of responsibility towards his friends.\\n\\n*   **Jean Tannen:** Locke's best friend and the muscle of the Gentleman Bastards. Jean is a skilled swordsman, a master strategist, and often the voice of reason in Locke's more\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" outlandish plans. He's fiercely protective of Locke and the others, and his loyalty is unwavering. Jean is also a scholar and a lover of books, providing a counterpoint to Locke's more impulsive nature. He's the most grounded and pragmatic of the group.\\n\\n*   **Calo Sanza and Galdo San\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\"za:** Twin brothers and integral members of the Gentleman Bastards. They are skilled actors and impersonators, essential for the gang's elaborate cons. Calo is generally considered the more outgoing and flamboyant of the two, while Galdo is quieter and more observant. They provide comic relief and are fiercely loyal to Locke and the others.\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" They are masters of disguise and deception.\\n\\n*   **Bug:** The youngest member of the Gentleman Bastards. He's an orphan taken in by Father Chains and trained in the art of thievery. Bug is eager to prove himself and looks up to Locke as a mentor. He's skilled at picking\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" pockets and navigating the city's underbelly. He represents the future of the Gentleman Bastards and is eager to learn.\\n\\n*   **Father Chains:** The enigmatic and ruthless leader of the Thiefmaker's Guild and Locke's mentor. He raised Locke and the other Gentleman Bastards, teaching them the skills they\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\" need to survive in the criminal underworld. Father Chains is a complex character with his own code of honor, but he's also willing to do whatever it takes to protect his own. He's a father figure to Locke, but also a manipulative and dangerous man.\\n\\nThese are the core characters that drive the narrative\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens': 0})}\n",
      "{'chunk': AIMessageChunk(content=\". Their relationships, loyalties, and conflicts are central to the story's themes of friendship, betrayal, and the cost of ambition. While other characters like Capa Barsavi, Don Salvara, and the elusive Sabetha are important, the Gentleman Bastards are the heart and soul of the book.\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', usage_metadata={'input_token_details': {'cache_read': 0}, 'output_tokens': 593, 'total_tokens': 584, 'input_tokens': -9})}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--f1a085ab-3d2e-439d-8eb9-6de4828ec012', chunk_position='last')}\n",
      "\n",
      " Total token chunks streamed: 15\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the main characters in the book The Lies of Locke Lamora\")\n",
    "token_count = 0\n",
    "\n",
    "async for event in graph.astream_events(\n",
    "    {\"messages\": [input_message]}, \n",
    "    config, \n",
    "    version=\"v2\"\n",
    "):\n",
    "    if (event[\"event\"] == \"on_chat_model_stream\" and \n",
    "        event['metadata'].get('langgraph_node','') ==node_to_stream):\n",
    "        token_count += 1\n",
    "        print(event[\"data\"])\n",
    "\n",
    "print(f\"\\n Total token chunks streamed: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay|, let'|s dive into the major characters of Scott Lynch's *The Lies of Locke| Lamora*. I'll cover their roles, personalities, and significant relationships:\n",
      "\n",
      "The Gentleman Bastards:**\n",
      "\n",
      " and exceptionally clever thief and con artist. He's known for his elaborate schemes, his love of fine things, and his deep loyalty to his friends. He|'s also plagued by self-doubt and a fear of failure, despite his outward confidence. Locke is the strategist and the face of the group. He's not a fighter, but he's a master of deception and manipulation.| He's driven by a desire for a better life for himself and his friends, and a love of the game.\n",
      "\n",
      " a skilled swordsman, a scholar, and the moral compass of the group. He's more pragmatic and cautious than Locke, often acting as a voice of reason. He's fiercely loyal to Locke and the others, and he's willing to do whatever it takes to protect them. Jean is also secretly a| romantic and has a deep love for books and learning. He provides a grounding influence on Locke's more outlandish plans.\n",
      "\n",
      " are jovial, quick-witted, and fiercely loyal to the group. They provide comic relief but are also capable and dangerous when needed. They are inseparable and often speak in unison or finish each other's sentences. They are the masters of disguise and performance.\n",
      "\n",
      " Gentleman Bastards. Bug is an apprentice thief and is eager to prove himself. He looks up to Locke and the others and is learning the tricks of the trade. He's naive and sometimes reckless, but he's also brave and resourceful. He represents the future of the Gentleman Bastards and the legacy they hope to leave|.\n",
      "\n",
      "**Other Important Characters:**\n",
      "\n",
      " loyalty, even within their criminal activities. He is deceased at the start of the book, but his influence is felt throughout, shaping their values and actions.as a child and taught him the art of thievery and deception. He instilled in them a code of honor and|\n",
      "\n",
      " controls the city's criminal activities. He is initially suspicious of Locke and the Gentleman Bastards but eventually becomes a reluctant ally, recognizing their skill and potential.\n",
      "\n",
      " seeks to overthrow the existing criminal hierarchy and establish his own rule. He is the primary antagonist of the book and a formidable opponent for Locke and his crew. He uses magical abilities and brutal tactics to achieve his goals, representing a threat unlike anything they've faced before.\n",
      "\n",
      " and one of Locke's targets. He is arrogant, greedy, and easily manipulated. He becomes embroiled in Locke's schemes and ultimately suffers the consequences, serving as a symbol of the corruption and excess that the Gentleman Bastards prey upon.\n",
      "\n",
      "**Key Relationships:**\n",
      "\n",
      " central relationship of the book. Their friendship is the heart of the Gentleman Bastards. They are fiercely loyal to each other and rely on each other for support and guidance. Their bond is tested throughout the story, but ultimately remains strong.\n",
      " and values. Father Chains taught Locke everything he knows and instilled in him a sense of honor and loyalty, even within a life of crime.\n",
      ". They are a found family, bound together by shared experiences and a common purpose.each other, protect each other, and share a deep sense of camaraderie|\n",
      "\n",
      "These are the major players in *The Lies of Locke Lamora*. Their interactions, motivations, and relationships drive the plot and make the book a compelling read. Remember that character development is a key element, and these characters evolve throughout the story.|\n",
      "|   - Total tokens: 22\n",
      "   - Total characters: 4573\n",
      "   - Average chars/token: 207.86\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the major characters in the lies of locke lamora\")\n",
    "\n",
    "accumulated_content = \"\"\n",
    "token_count = 0\n",
    "\n",
    "async for event in graph.astream_events(\n",
    "    {\"messages\": [input_message]}, \n",
    "    config, \n",
    "    version=\"v2\"\n",
    "):\n",
    "    if (event[\"event\"] == \"on_chat_model_stream\" and \n",
    "        event['metadata'].get('langgraph_node','') == node_to_stream):\n",
    "        data = event[\"data\"]\n",
    "        content = data[\"chunk\"].content\n",
    "        accumulated_content += content\n",
    "        token_count += 1\n",
    "        print(content, end=\"|\", flush=True)\n",
    "\n",
    "print(f\"   - Total tokens: {token_count}\")\n",
    "print(f\"   - Total characters: {len(accumulated_content)}\")\n",
    "print(f\"   - Average chars/token: {len(accumulated_content)/token_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82ca2300-c889-4b61-a794-a71f969ca0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Streaming Mode Comparison\n",
      "\n",
      "============================================================\n",
      "\n",
      " Mode: values\n",
      "----------------------------------------\n",
      "Events: 2, Time: 11.010s\n",
      "\n",
      " Mode: updates\n",
      "----------------------------------------\n",
      "Events: 1, Time: 10.685s\n",
      "\n",
      "============================================================\n",
      " Comparison Summary:\n",
      "   values    : 2 events in 11.010s\n",
      "   updates   : 1 events in 10.685s\n"
     ]
    }
   ],
   "source": [
    "# Compare streaming modes side by side\n",
    "async def compare_streaming_modes(input_msg):\n",
    "    \"\"\"Compare different streaming modes.\"\"\"\n",
    "    \n",
    "    print(\" Streaming Mode Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    modes = [\"values\", \"updates\"]\n",
    "    results = {}\n",
    "    \n",
    "    for mode in modes:\n",
    "        print(f\"\\n Mode: {mode}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        config = {\"configurable\": {\"thread_id\": f\"compare_{mode}\"}}\n",
    "        start_time = time.time()\n",
    "        event_count = 0\n",
    "        \n",
    "        for event in graph.stream(\n",
    "            {\"messages\": [input_msg]},\n",
    "            config,\n",
    "            stream_mode=mode\n",
    "        ):\n",
    "            event_count += 1\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        results[mode] = {\n",
    "            \"events\": event_count,\n",
    "            \"time\": elapsed\n",
    "        }\n",
    "        \n",
    "        print(f\"Events: {event_count}, Time: {elapsed:.3f}s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Comparison Summary:\")\n",
    "    for mode, stats in results.items():\n",
    "        print(f\"   {mode:10}: {stats['events']} events in {stats['time']:.3f}s\")\n",
    "\n",
    "# Run comparison\n",
    "await compare_streaming_modes(HumanMessage(content=\"Who are the major characters in the book the stormlight archive? what are the major themes?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "** Notice**\n",
    "\n",
    "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
    "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "-  API: http://127.0.0.1:2024\n",
    "-  Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "-  API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the **Studio UI** URL shown above.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '019a260b-545b-737e-9418-039a8e604014', 'attempt': 1})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '6e679f4e-e170-4721-8f44-7d6212abe762'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '6e679f4e-e170-4721-8f44-7d6212abe762'}, {'content': '2 multiplied by 3 is 6.\\n', 'additional_kwargs': {}, 'response_metadata': {'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'model_provider': 'google_genai'}, 'type': 'ai', 'name': None, 'id': 'lc_run--40a1c3f8-9425-4ea9-a21e-6005df4744c9', 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}}], 'message_count': 1})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"streaming\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='50ccf930-74ee-4e49-a87e-73c603211188'\n",
      "=========================\n",
      "content='2 multiplied by 3 is 6.\\n' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}}} response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'model_provider': 'google_genai'} id='lc_run--c05639c4-ef85-4e68-a99c-ce4f9bcd6830'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"streaming\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 019a0358-57dc-76f9-bc63-633eee467a86\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "--------------------------------------------------\n",
      "AI: The result\n",
      "--------------------------------------------------\n",
      "AI: The result of\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata and response_metadata.get(\"finish_reason\"):\n",
    "                    print(f\"Response Metadata: Finish Reason - {response_metadata['finish_reason']}\")                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
